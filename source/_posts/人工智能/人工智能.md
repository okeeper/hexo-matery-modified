---
title: 人工智能技术发展史
date: 2023-7-30 11:00:00
author: okeeper
top: true
toc: true
cover: true
summary: 指由人制造出来的机器所表现出来的智能。*人工智能*可以定义为模仿人类与人类思维相关的认知功能的机器或计算机，如学习和解决问题
categories: 人工智能
tags:
  - 人工智能
  - AI	
  - 大语言模型
  - LLM
  - 神经网络
---

# 人工智能

# 什么是人工智能

指由人制造出来的机器所表现出来的智能。*人工智能*可以定义为模仿人类与人类思维相关的认知功能的机器或计算机，如学习和解决问题

- 专家系统：基于专家知识库+推理引擎。比如使用预定的一些规则和逻辑，模拟人类专家进行决策的过程
- 启发式问题解决：比如评估小范围的解决方案，并可能涉及一些猜测，以找到接近最佳的解决方案。
- 自然语言处理：在自然语言中实现人机之间的交流。比如聊天机器人。
- 计算机视觉：自动生成识别形状和功能的能力。比如人脸识别、图像识别、语音识别（本质还是基于数据化之后的语音图谱）



# 如何实现这些智能



## 手动编写规则和逻辑

手动编写各种规则，以响应特定的输出，类似IF/ELSE逻辑，更加复杂的就是一些数学函数规则。

比如情感问题专家系统，有一个预设好的问题和答案库，当用户输入某个问题时，返回对应问题的答案。

比如在早期的机器翻译，就是通过语法规则解析+关键词匹配来识别和理解用户的输入，并响应对应目标语言的语法规则和关键词生成翻译语言。

在比如我们公司的风控系统，根据输入特征结合预设规则返回风控结果等等。在一定程度上都能实现简单的类人类的一些智能。



## 基于统计概率

基于大量实际的例子进行概率统计预测，例如我们的手机输入法，个人常用的词组将会放在前面。



人们很快发现，对于实世界的复杂问题并不都可以通过预设规则很好地处理，

比如人脸识别、图像识别，一个人从早到晚、每天的情绪变化都会细微地影响你的人脸构成，而计算机又是极其“死脑筋”的，任何细微的变化都可能对同一个人识别失败，所以我们不可能通过预设所有情况很好地解决这类问题。



再比如自然语言处理，只通过预发解析和关键词匹配的出来的解释并不是一成不变的，语言中有很多歧义性(多义词)、容错性（错别字、顺序错误、语法错误）。这样下来我们实际使用的语言是灵活性极高的，并不能穷举完所有情况进行预设。例如下面这段话：

> 他说：“她这个人真有意思(funy)。”她说：“他这个人怪有意思的(funy)。”于是人们以为他们有了意思(wish)，并让他向她意思意思（express）。他火了：“我根本没有那个意思(thought)！”她也生气了：“你们这么说是什么意思（Intention）？”事后有人说：“真有意思(funny)。”也有人说：“真没意思(nonsense)。”

很难通过预设的一些语法和关键词进行很好的翻译对应的“意思”。当然自然语言处理发展已有半个世纪的发展。在机器学习之前的翻译翻译效果也一直不够理想。

![image-20240219163614202](https://okeeper-blog-images.oss-cn-hangzhou.aliyuncs.com/images/image-20240219163614202.png)



## 机器学习

利用数据和算法让计算机自动学习模式、规律，并进行预测和分类。

就比如给一堆的数据：标注好的图片、标注好的语料。让计算机自动通过机器学习的过程不断总结规律直至预测和分类效果达到可接受的范围之内。从而得到能够预测和分类未知输入的能力。

过程类似这样。

![image-20240219163634073](https://okeeper-blog-images.oss-cn-hangzhou.aliyuncs.com/images/image-20240219163634073.png)





就是让机器通过数学建模找到f(x)函数，使得他可以拟合我们现实世界中的任意复杂场景。

例如这样的线性函数，找到个f(x)函数使得到各个点的实际误差最小。



例如这样的类一元二次函数的点分布，找到一个抛物线使得到各个点的误差最小。



我们人类可以根据数学经验和已知的一些规律找到对于函数类型，是一元一次方程、一元二次方程、或者是指数函数，正余弦等等。

但是对于计算机来讲他怎么知道该用什么类型的函数来拟合呢。如果数据分布呈现不了任何规律，又改如何找对对于的函数进行拟合呢。

比如如这样的，这样的，甚至这样的。



## 神经网络

这个东西也叫通用函数逼近器，数学家已经证明，两层的神经网络足以拟合任何复杂函数。



## 神经网络的类型

CNN

RNN

DNN



## 一个完整模型训练(“炼丹”)的过程

无监督学习

有监督学习

微调



参考文献：

小白也能听懂的人工智能原理：https://www.aliyundrive.com/s/hXdxXyuuzdW 提取码p2z7



 Transformer的基本原理：https://blog.51cto.com/u_16161414/6483603



神经网络的基本原理：https://blog.51cto.com/u_16161414/6479769



神经网络初探——神经网络如何工作：https://zhuanlan.zhihu.com/p/64532465



一文看懂四种基本的神经网络架构：https://www.jianshu.com/p/546dc40b52c8



很好的大语言模型的入门：ChatGPT背后的语言模型简史：https://www.bmpi.dev/dev/deep-learning/nlp-language-models/



图解Word2vec,是如何训练出来的：https://mp.weixin.qq.com/s/NMngfR7EWk-pa6c4_FY9Yw



如何通俗理解Word2Vec (23年修订版)

https://blog.csdn.net/v_JULY_v/article/details/102708459



 Transformer通俗笔记：从Word2Vec、Seq2Seq逐步理解到GPT、BERT：https://blog.csdn.net/v_JULY_v/article/details/127411638



图解GPT：https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/2.4-%E5%9B%BE%E8%A7%A3GPT.md



超大型人工智能：从GPT->GPT2->GPT3的发展历程+大规模预训练神经网络模型原理详解： https://zhuanlan.zhihu.com/p/591146772



提示词工程：https://www.51cto.com/article/749832.html