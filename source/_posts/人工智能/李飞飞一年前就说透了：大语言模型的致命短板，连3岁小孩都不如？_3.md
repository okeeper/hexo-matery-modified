---
title: 李飞飞一年前就说透了：大语言模型的致命短板，连3岁小孩都不如？
date: 2025-09-12 19:22:38
author: okeeper
top: false
toc: true
categories: 人工智能
tags:
  - 人工智能
  - AI
---

最近AI圈又在热议一个"老话题"——大语言模型到底算不算真智能？起因是李飞飞一年前的一段访谈被扒了出来，她的观点现在看依然扎心："大自然中可没有语言，你不会从天空中直接看到文字。语言是纯粹的生成信号，而物理世界是客观存在的。"

这话翻译成人话就是：现在的大模型天天在文字堆里打转，看似啥都懂，其实对真实世界的物理常识可能还不如个三岁小孩。

## 语言模型的"一维困境"：用文字理解3D世界就像盲人摸象

李飞飞在访谈里点出了一个核心矛盾：现在的大语言模型，包括那些号称"多模态"的，骨子里还是在玩"一维游戏"。

![语言本质讨论](https://okeeper-blog-images.oss-cn-hangzhou.aliyuncs.com/blog-images/202509/9754f12eb4d60332c8964f1d0c613a51.gif)

她解释说："这些模型的底层表示是一维的，它们操作的是离散token的一维序列。处理书面文本没问题，但物理世界是三维的啊！"

你想想，我们人类感知世界是全方位的：眼睛看颜色形状，耳朵听声音方向，手摸软硬冷热，这些信号同时涌入大脑，形成对3D世界的认知。而大模型呢？即便给它图片，最终也得把像素转换成一维的token序列塞进模型，这不就像把立体世界硬生生压成了一张平面画吗？

![3D数据处理讨论](https://okeeper-blog-images.oss-cn-hangzhou.aliyuncs.com/blog-images/202509/bca393da2ea671a5be03d10c77f92797.gif)

更关键的是，李飞飞强调："语言信号的输出主要基于人类给的输入信号，它不独立于人之外。但物理世界是客观存在的，不管有没有人类观察，苹果都会往下掉，而不是往上飞。"

说白了，大模型学到的可能只是"人类怎么描述物理现象"，而不是"物理现象本身如何运作"。这就好比你背熟了菜谱，但从没下过厨，遇到食材变质、火候变化这些实际情况，照样抓瞎。

## 残酷测试：最牛大模型的物理常识，连幼儿园水平都达不到？

光说不练假把式，科学家们早就动手测试大模型的"真实智商"了。结果嘛...有点惨不忍睹。

### 测试一：3D环境里找小球，模型被障碍物绕晕

有个研究团队搞了个叫"Animal AI Environment"的3D虚拟环境，让大模型控制一个小角色完成各种物理任务。简单说就是让AI在游戏里"生活"，看看它会不会像人一样处理日常物理问题。

![AI环境测试流程](https://okeeper-blog-images.oss-cn-hangzhou.aliyuncs.com/blog-images/202509/d5059ac3b6d00b55b55ae0a3348d4457.png)

测试对象包括GPT-4o、Claude 3.5 Sonnet、Gemini 1.5 Pro这些顶级选手，还给它们找了个对照组——人类儿童。

最简单的任务：直接找到房间里的小球。模型们表现还行，勉强过关。

稍微增加难度：房间里放个障碍物，得饶过去才能拿到球。结果呢？模型们瞬间懵圈，要么对着障碍物发呆，要么绕远路跑到姥姥家。

![物理常识能力对比](https://okeeper-blog-images.oss-cn-hangzhou.aliyuncs.com/blog-images/202509/19e12834b86da564c652344ff297b1d7.png)

从数据看，在12项物理常识任务中，最厉害的模型平均通过率也就刚过50%，而人类儿童随随便便就能达到85%以上。更扎心的是，研究人员还特意给模型"上课"，演示正确做法，结果模型表现几乎没提升——它好像根本没理解"为什么要这么做"。

### 测试二：改个数字就露馅，物理题正确率暴跌22%

浙江大学和蚂蚁集团的团队更狠，直接上"物理考卷"——设计了个叫ABench-Physics的测试集，专门考大模型物理推理。

![物理推理测试研究](https://okeeper-blog-images.oss-cn-hangzhou.aliyuncs.com/blog-images/202509/2219b946a8bd5392403004f7261f8820.png)

考卷分两部分：
- Phy A：400道静态物理题，类似中学物理竞赛题
- Phy B：把Phy A的题目数字改一下（比如把"质量5kg"改成"质量8kg"），但物理原理不变

结果呢？最牛的模型在Phy A上正确率刚到43%，也就勉强及格的水平。到了Phy B，平均正确率直接暴跌22.5%！

![物理推理测试结果](https://okeeper-blog-images.oss-cn-hangzhou.aliyuncs.com/blog-images/202509/a06724cd4b7f89cb7d03ebaf9af3b0e7.png)

这说明啥？模型可能是在"背题"，记住了特定数字的答案，而不是理解了F=ma这些物理公式的本质。就像有些学生考试靠死记硬背，题目稍微变个说法就彻底不会了。

### 测试三：看图辨远近，模型正确率还不到人类一半

语言不行，那视觉呢？毕竟现在都是"多模态大模型"了。

斯坦福团队做了个视觉感知测试：给模型看照片，让它判断哪个物体更近；或者看一堆碎片，让它拼出完整物体。这些对人类来说都是幼儿园水平的任务。

![视觉感知研究论文](https://okeeper-blog-images.oss-cn-hangzhou.aliyuncs.com/blog-images/202509/6e014c9e125fe1a24b6afe353098ec84.png)

结果人类正确率95.7%，而几个主流大模型最高才51%，最低的只有32%。想象一下，你给AI看张风景照，问"山近还是房子近"，它抛硬币猜答案的概率都比这高。

## 争论升级：语言和物理，到底谁更能"编码"现实？

李飞飞的观点和这些测试结果在网上炸开了锅，网友们吵翻了天。

支持派觉得说到了点子上："LLMs本质上是在操作人类生成的信号系统，而不是直接理解物理世界。就像你可以通过书本学习游泳理论，但不下水永远学不会游泳。"

![语言信号系统推文](https://okeeper-blog-images.oss-cn-hangzhou.aliyuncs.com/blog-images/202509/0844a5409af0c108edd3270fbcb6a517.png)

甚至有人预测："纯语言模型的时代快结束了，视觉语言模型(VLMs)才是未来，比如GPT-4o、Gemini这些已经在往这个方向走。"

但反对派也不甘示弱："语言怎么就不重要了？人类文明不就是靠语言传承的吗？牛顿三大定律不也是用语言写出来的？"


还有个有意思的观点："语言和物理都是编码现实的方式。有时候语言描述现实的能力甚至比物理学还强——你能用'爱'这个词描述一种复杂情感，但物理学公式能吗？"

![语言编码现实观点](https://okeeper-blog-images.oss-cn-hangzhou.aliyuncs.com/blog-images/202509/6c6a2a318354486945804f34b3068f1e.png)

我觉得吧，这场争论的核心不是"语言vs物理"谁更重要，而是"如何让AI同时掌握这两种编码方式"。就像人类既需要语言交流，也需要亲身体验世界，两者缺一不可。

## 未来的路：从"纸上谈兵"到"动手实践"

现在的大模型有点像"只会纸上谈兵的赵括"，背了一堆知识，但不会灵活运用。要突破这个瓶颈，可能需要两条腿走路：

一方面，得让模型真正"接触"物理世界——不是通过文字描述，而是通过传感器、机器人等实体，亲自"摸一摸"、"动一动"。就像教小孩认识苹果，与其给他看100张苹果图片，不如直接给他个苹果让他摸、闻、尝。

另一方面，研究人员也在探索新的模型架构，能不能让模型的底层表示更接近3D物理世界，而不是局限于一维的文字序列。比如把空间位置、物理属性这些信息直接编码进模型，而不是让它从文字中间接"猜"这些属性。

不过话说回来，这些都不是短期内能搞定的。大语言模型用了十年才发展到今天的水平，要让AI真正理解物理世界，可能还需要更长的时间。

最后想问一句：如果AI有一天真的能像人类一样理解物理世界，它会不会反问我们："你们人类为什么花了几千年才搞懂引力？这不是很简单吗？"